# General benchmark settings.
[benchmark]
seed = 42
deterministic = true  # Can slow down the process
exist_pass = true
img_format = ".jpg"
hypertune_trials = 700
ignore_optimizers = [
  "msvag",
  "lbfgs",
  "lomo",
  "adalomo",
  "demo",
  "a2grad",
  "muon",
  "alice",
  "adamc",
  "adamwsn",
  "adamuon",
  "splus",
  "alig",
  "bsam",
  "distributedmuon",
  "sgdw",
  "adamw",
  "stableadamw",
  "dadaptadam",
  "amos",
]

# Configuration for each benchmark function, including iterations and error weight.
[functions.Ackley]
iterations = 150
error_weight = 1.0

[functions."Cross-in-Tray"]
iterations = 150
error_weight = 0.5

[functions.EggHolder]
iterations = 150
error_weight = 1.0

[functions."Gramacy-Lee 2D"]
iterations = 100
error_weight = 2.5

[functions.Griewank]
iterations = 200
error_weight = 1.0

[functions."Holder Table"]
iterations = 150
error_weight = 1.5

[functions.Langermann]
iterations = 150
error_weight = 1.0

[functions.Levy]
iterations = 150
error_weight = 1.0

[functions."Levy 13"]
iterations = 150
error_weight = 1.0

[functions.Rastrigin]
iterations = 150
error_weight = 1.0

[functions.Rosenbrock]
iterations = 400
error_weight = 1.0

[functions."Schaffer 2"]
iterations = 500
error_weight = 1.8

[functions."Schaffer 4"]
iterations = 500
error_weight = 1.8

[functions.Shubert]
iterations = 150
error_weight = 1.0

[functions."Drop-Wave"]
iterations = 150
error_weight = 1.0

[functions."Styblinski-Tang"]
iterations = 150
error_weight = 1.0

# Hyperparameter search spaces for optimizers.
# If an optimizer is not listed here, the 'default' space is used.
[hyperparameters]
default = { lr = [0.0001, 2.0] }
adadelta = { lr = [0.0001, 600.0] }
adafactor = { lr = [0.0001, 600.0] }
adamg = { lr = [0.0001, 20.0], p = [0, 1], q = [0, 1] }
adams = { lr = [0.0001, 10.0] }
dadaptadagrad = { lr = [0.0001, 20.0] }
dadaptlion = { lr = [0.0001, 10.0] }
padam = { lr = [0.0001, 10.0] }
adahessian = { lr = [0.0001, 100.0] }
sophiah = { lr = [0.0001, 60.0] }
pid = { lr = [0.0001, 0.5], derivative = [2.0, 14.0], integral = [1.0, 10.0], momentum = [
  0.0,
  0.99,
] }
tam = { lr = [0.0001, 6.0], momentum = [0.0001, 0.99] }
sgdp = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
accsgd = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
sgdw = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
signsgd = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
sgdsai = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
sgd = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
alig = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
asgd = { lr = [0.0001, 2.0], amplifier = [0.0001, 0.5] }
amos = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
schedulefreesgd = { lr = [0.0001, 3.0], momentum = [0.0001, 0.99] }
schedulefreeradam = { lr = [1.0, 10.0] }
kron = { lr = [0.0001, 2.0], momentum = [0.0001, 0.99] }
scion = { lr = [0.0001, 1.0], scale = [1.0, 1000.0] }
scionlight = { lr = [0.0001, 1.0], scale = [1.0, 1000.0] }

# Special evaluation arguments for specific optimizers.
# These are passed to the `execute_steps` function.
[optimizer_eval_args]
alig = { use_closure = true }
bsam = { use_closure = true }
lbfgs = { use_closure = true }
adahessian = { use_graph = true }
sophiah = { use_graph = true }

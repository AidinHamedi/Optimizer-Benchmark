# ==============================================================================
# Global Benchmark Settings
# ==============================================================================
[benchmark]
seed = 42
exist_pass = true
img_format = "jpg"
hypertune_trials = 800
ignore_optimizers = [
  # Unsupported
  "msvag",
  "lomo",
  "adalomo",
  "demo",
  "a2grad",
  "muon",
  "adago",
  "alice",
  "adamwsn",
  "adamuon",
  "splus",
  "alig",
  "bsam",
  "distributedmuon",
  "sgdw",
  "adamw",
  "stableadamw",
  "dadaptadam",
  "dadaptsgd",
  "amos",
  # Not working
  "qhadam",
  "shampoo",
  "mars",
  "adafactor",
  "adamg",
  "schedulefreeradam",
  "srmm",
  "adamod",
  "adams",
  "dadaptadan",
  "lbfgs",
]

# ==============================================================================
# Test Function Configurations
# Iterations: number of optimization steps per function
# Error Weight: importance multiplier for final scoring (higher = more impact)
# ==============================================================================
[functions]
"Gramacy & Lee 2D" = { iterations = 100, error_weight = 2.0 }
Weierstrass = { iterations = 300, error_weight = 2.0 }
"Goldstein-Price" = { iterations = 150, error_weight = 2.0 }
Ackley = { iterations = 150, error_weight = 2.0 }
NeuralCanyon = { iterations = 150, error_weight = 2.0 }
Rosenbrock = { iterations = 400, error_weight = 2.0 }
GradientLabyrinth = { iterations = 250, error_weight = 2.0 }
Rastrigin = { iterations = 150, error_weight = 1.5 }
Griewank = { iterations = 200, error_weight = 1.5 }
"LÃ©vy 13" = { iterations = 150, error_weight = 1.5 }
MixedMichalewiczSphere = { iterations = 150, error_weight = 1.0 }
Langermann = { iterations = 150, error_weight = 0.5 }
"Langermann 2" = { iterations = 150, error_weight = 0.5 }
"Styblinski-Tang" = { iterations = 150, error_weight = 0.5 }
QuantumWell = { iterations = 150, error_weight = 0.5 }
EggHolder = { iterations = 150, error_weight = 0.2 }
"Drop-Wave" = { iterations = 150, error_weight = 0.2 }

# ==============================================================================
# Optimizer Hyperparameter Search Spaces
# Format: parameter = [min, max] for floats, [min, max, "int"] for integers
# _iter_scale: multiplier for iteration count (not a hyperparameter)
# ==============================================================================
[optimizers]
base = { lr = [0.0001, 8.0] }

padam = { lr = [0.0001, 12.0] }
dadaptlion = { lr = [0.0001, 12.0] }
sophiah = { lr = [0.0001, 50.0] }
adadelta = { lr = [0.0001, 500.0] }
dadaptadagrad = { lr = [0.0001, 20.0] }
adahessian = { lr = [0.0001, 50.0] }

sgd = { lr = [0.0001, 4.0], momentum = [0.0001, 0.99] }
sgdp = { lr = [0.0001, 4.0], momentum = [0.0001, 0.99] }
sgdsai = { lr = [0.0001, 4.0], momentum = [0.0001, 0.99] }
accsgd = { lr = [0.0001, 4.0], momentum = [0.0001, 0.99] }
signsgd = { lr = [0.0001, 4.0], momentum = [0.0001, 0.99] }
tam = { lr = [0.0001, 6.0], momentum = [0.0001, 0.99] }
schedulefreesgd = { lr = [0.0001, 6.0], momentum = [0.0001, 0.99] }
kron = { lr = [0.0001, 4.0], momentum = [0.0001, 0.99] }
pid = { lr = [0.0001, 4.0], momentum = [0.0, 0.99], integral = [1.0, 10.0], derivative = [2.0, 14.0] }

asgd = { lr = [0.0001, 6.0], amplifier = [0.0001, 0.5] }

scion = { lr = [0.0001, 4.0], scale = [1.0, 1000.0] }
scionlight = { lr = [0.0001, 4.0], scale = [1.0, 1000.0] }

conda = { lr = [0.0001, 6.0], update_proj_gap = [10, 400, "int"], scale = [0.0001, 12.0] }

grokfastadamw = { lr = [0.0001, 4.0], _iter_scale = 7 }

# ==============================================================================
# Special Evaluation Arguments
# use_closure: optimizer requires a closure function
# use_graph: optimizer requires create_graph=True in backward()
# ==============================================================================
[optimizer_eval_args]
alig = { use_closure = true }
bsam = { use_closure = true }
lbfgs = { use_closure = true }
adahessian = { use_graph = true }
sophiah = { use_graph = true }
